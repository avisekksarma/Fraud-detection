{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avisekksarma/Fraud-detection/blob/main/fe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJEaB7VjLw6_",
        "outputId": "d49808c6-083f-47a9-a790-6a746524b65a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive' , force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSW4uJuKtEWt"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qcdTMUULwqg",
        "outputId": "891e726a-4400-477c-d4a6-fadda8d495da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "os.chdir('//content/gdrive/My Drive/Fraud_detection/')\n",
        "\n",
        "print('Load Data')\n",
        "train_df = pd.read_pickle('train_transaction.pkl')\n",
        "test_df = pd.read_pickle('test_transaction.pkl')\n",
        "test_df['isFraud'] = 0\n",
        "\n",
        "train_identity = pd.read_pickle('train_identity.pkl')\n",
        "test_identity = pd.read_pickle('test_identity.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load Data\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d575ba28cdf7>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Load Data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_transaction.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_transaction.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'isFraud'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \"\"\"\n\u001b[1;32m    189\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_transaction.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBHw3cATM5qJ"
      },
      "source": [
        "card4,card6 and ProductCD are frequency encoded (prior)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nf1wKI8NxVQ"
      },
      "source": [
        "# train_df[['card1','card6','ProductCD']].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4MuIL_gOxk2"
      },
      "source": [
        "M4 is frequency encoded . M1-M9 values describe card match features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CAg8CEDMu5B"
      },
      "source": [
        "# train_df.loc[:, 'M1':'M9'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBuz9-ZKVyco"
      },
      "source": [
        "# Here I shall do Feature Engineering and implement a baseline model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m49L28egJJEs"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiWXTfnqLDgA"
      },
      "source": [
        "In many practical data science activities, the data set will contain categorical variables. These variables are typically stored as text values”. Since machine learning is based on mathematical equations, it would cause a problem when we keep categorical variables as is. Many algorithms support categorical values without further manipulation, but in those cases, it’s still a topic of discussion on whether to encode the variables or not. The algorithms that do not support categorical values, in that case, are left with encoding methodologies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flznMoQ_LNuv"
      },
      "source": [
        "#### **FE and Model**\n",
        "\n",
        "Crucial steps\n",
        "\n",
        "1.Make ground baseline with no fe <br>\n",
        "2.Make a small FE and see I you can understand data you have <br>\n",
        "3.Find good CV strategy <br>\n",
        "4.Feature selection <br>\n",
        "5.Make deeper FE <br>\n",
        "6.Tune Model (crude tuning) <br>\n",
        "7.Try other Models (never forget about NN) <br>\n",
        "8.Try Blending/Stackin/Ensembling <br>\n",
        "9.Final tuning <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4teZluUlU_j4"
      },
      "source": [
        "##### **1.BaseLine Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Lyjhj5vLd8M"
      },
      "source": [
        "# 1. Baseline Model . It doesn't really matter what model to use her. Make ground baseline. No feature engineering. No tuning.\n",
        "\n",
        "train = train_df.merge(train_identity, how='left', left_index=True, right_index=True)\n",
        "test = test_df.merge(test_identity, how='left', left_index=True, right_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHx79eoFTwUH"
      },
      "source": [
        "# print(train.shape)\n",
        "# print(test.shape)\n",
        "\n",
        "target = train['isFraud'].copy()\n",
        "X_train = train.drop('isFraud', axis=1)\n",
        "X_train.drop('TransactionDT', axis=1, inplace=True)\n",
        "X_test = test.drop('TransactionDT', axis=1)\n",
        "\n",
        "# Drop target, fill in NaNs\n",
        "\n",
        "X_test = test.copy()\n",
        "X_test.columns = X_test.columns.str.replace('-','_')\n",
        "\n",
        "# If you give np.nan to LGBM, then at each tree node split,\n",
        "# it will split the non-NAN values and then send all the NANs to either the left child or right child depending on what’s best.\n",
        "# Therefore NANs get special treatment at every node and can become overfit.\n",
        "# By simply converting all NAN to a negative number lower than all non-NAN values (such as - 999),\n",
        "\n",
        "X_train = X_train.fillna(-999)\n",
        "X_test = X_test.fillna(-999)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ-DmmNKUy3G"
      },
      "source": [
        "del train,test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSel4OC2r2nb"
      },
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU-g49ryrMdf"
      },
      "source": [
        "# X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmiVrtS5nO8L"
      },
      "source": [
        "# X_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOPkFyULZuyC"
      },
      "source": [
        "columns = []\n",
        "for f in X_train.columns:\n",
        "  if X_train[f].dtype == 'object':\n",
        "    columns.append(f)\n",
        "\n",
        "print(columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_Xe42oHVUDw"
      },
      "source": [
        "#Frequency Encoding\n",
        "\n",
        "def frequency_encoding(train_df, test_df, columns,self_encoding=True):\n",
        "    for col in columns:\n",
        "        temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
        "        fq_encode = temp_df[col].value_counts(dropna=False).to_dict()\n",
        "\n",
        "        if self_encoding:\n",
        "            train_df[col] = train_df[col].map(fq_encode)\n",
        "            test_df[col]  = test_df[col].map(fq_encode)\n",
        "        else:\n",
        "            train_df[col+'_fq_enc'] = train_df[col].map(fq_encode)\n",
        "            test_df[col+'_fq_enc']  = test_df[col].map(fq_encode)\n",
        "    return train_df, test_df\n",
        "\n",
        "X_train,X_test = frequency_encoding(X_train,X_test , columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E20ceY7-1h6I"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neGCQ6YH1l9j"
      },
      "source": [
        "X_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odCJUax3oay4"
      },
      "source": [
        "# X_train = X_train.drop(['TransactionID_x'],axis=1)\n",
        "# X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWv00R2xoDME"
      },
      "source": [
        "# X_test = X_test.drop(['TransactionID_x'],axis=1)\n",
        "# X_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxB6aq7nsq6L"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrqUkXJvsq3D"
      },
      "source": [
        "SEED = 10\n",
        "\n",
        "params = {'num_leaves': 256,\n",
        "          'min_child_samples': 79,\n",
        "          'objective': 'binary',\n",
        "          'max_depth': 13,\n",
        "          'learning_rate': 0.03,\n",
        "          \"boosting_type\": \"gbdt\",\n",
        "          \"subsample_freq\": 3,\n",
        "          \"subsample\": 0.9,\n",
        "          \"bagging_seed\": 11,\n",
        "          \"metric\": 'auc',\n",
        "          \"verbosity\": -1,\n",
        "          'reg_alpha': 0.3,\n",
        "          'reg_lambda': 0.3,\n",
        "          'colsample_bytree': 0.9,\n",
        "          #'categorical_feature': cat_cols\n",
        "         }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuY7Z1_Osq0q"
      },
      "source": [
        "splits = 5\n",
        "folds = KFold(n_splits = splits)\n",
        "oof = np.zeros(len(X_train))\n",
        "predictions = np.zeros(len(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRy3X1PWsqxo"
      },
      "source": [
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train.values, target.values)):\n",
        "    print(\"Fold {}\".format(fold_))\n",
        "    train_df, y_train_df = X_train.iloc[trn_idx], target.iloc[trn_idx]\n",
        "    valid_df, y_valid_df = X_train.iloc[val_idx], target.iloc[val_idx]\n",
        "\n",
        "    trn_data = lgb.Dataset(train_df, label=y_train_df)\n",
        "    val_data = lgb.Dataset(valid_df, label=y_valid_df)\n",
        "\n",
        "    clf = lgb.train(params,\n",
        "                    trn_data,\n",
        "                    10000,\n",
        "                    valid_sets = [trn_data, val_data],\n",
        "                    verbose_eval=500,\n",
        "                    early_stopping_rounds=500)\n",
        "\n",
        "    pred = clf.predict(valid_df)\n",
        "    oof[val_idx] = pred\n",
        "    print( \"  auc = \", roc_auc_score(y_valid_df, pred) )\n",
        "    predictions += clf.predict(X_test) / splits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ5UBzq-eIh-"
      },
      "source": [
        "##### **2.Drop Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCS9-gLpeHbE"
      },
      "source": [
        "#TODO :\n",
        "# 1. view Nans and drop those with 70% nans or else fill nans\n",
        "# 2. drop redundant features check corelation and remove\n",
        "# 3. drop less important features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYNnGd4OiSJU"
      },
      "source": [
        "Aggregation ? <br>\n",
        "D Column <br>\n",
        "Clip Transaction Amount <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VXoohBrhkY-"
      },
      "source": [
        "##### **3.Normalize Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHaqTQUviHIJ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVRaqLlh_Eu"
      },
      "source": [
        "##### **4.Outlier Removal**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpSNqhQ7hpBL"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G443etqj_xc"
      },
      "source": [
        "##### **5.PCA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG3TjnBokHSO"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlPkLkGYjqLY"
      },
      "source": [
        "##### **5.CV strategy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aya8--0FjwXi"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}